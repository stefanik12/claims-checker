{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/xstefan3/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')    # Download vocabulary from S3 and cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/xstefan3/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')    # Download model and configuration from S3 and cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/xstefan3/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', 'bert-base-uncased')    # Download model and configuration from S3 and cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/xstefan3/.cache/torch/hub/huggingface_pytorch-transformers_master\n",
      "Using cache found in /home/xstefan3/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single-sample example of classification using an input pair\n",
    "\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', 'bert-base-cased')\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased-finetuned-mrpc')\n",
    "\n",
    "text_1 = \"Jim Henson was a puppeteer\"\n",
    "text_2 = \"Who was Jim Henson ?\"\n",
    "indexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "segments_tensors = torch.tensor([segments_ids]*3)\n",
    "tokens_tensor = torch.tensor([indexed_tokens]*3)\n",
    "\n",
    "# Predict the sequence classification logits\n",
    "with torch.no_grad():\n",
    "    seq_classif_logits = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    \n",
    "predicted_labels = torch.argmax(seq_classif_logits[0]).item()\n",
    "\n",
    "# assert predicted_labels == 0  # In MRPC dataset this means the two sentences are not paraphrasing each other\n",
    "\n",
    "# Or get the sequence classification loss (set model to train mode before if used for training)\n",
    "labels = torch.tensor([1, 1, 1])\n",
    "seq_classif_loss = model(tokens_tensor, token_type_ids=segments_tensors, labels=labels)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0723,  0.3583],\n",
       "        [-0.0723,  0.3583],\n",
       "        [-0.0723,  0.3583]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_classif_logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xstefan3/bert_experiments/pytorch\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>answer</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>statue of Mary.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>Mary reputedly appeared to Saint</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  To whom did the Virgin Mary allegedly appear i...   \n",
       "2  What is in front of the Notre Dame Main Building?   \n",
       "3  What is in front of the Notre Dame Main Building?   \n",
       "4  The Basilica of the Sacred heart at Notre Dame...   \n",
       "\n",
       "                                           paragraph  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                             answer  positive  \n",
       "0        Saint Bernadette Soubirous         1  \n",
       "1                   statue of Mary.         0  \n",
       "2         a copper statue of Christ         1  \n",
       "3  Mary reputedly appeared to Saint         0  \n",
       "4                 the Main Building         1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# customized squad dataset\n",
    "# classification of a piece of text being an answer to any question\n",
    "# we consider an answer to the question as a quatified information\n",
    "import pandas as pd\n",
    "df = pd.read_pickle(\"cc/1_extraction/quantified_statement_pairs.df\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize our pairs to MRPC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>175198.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            positive\n",
       "count  175198.000000\n",
       "mean        0.500000\n",
       "std         0.500001\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.500000\n",
       "75%         1.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_mrpc_i = df.groupby(\"paragraph\").count()[\"question\"].reset_index()\n",
    "df_mrpc_i[\"i\"] = df_mrpc.index.tolist()\n",
    "df_mrpc_i.set_index(\"paragraph\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18890"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"#1 ID\"] = df[\"paragraph\"].apply(lambda p: df_mrpc_i.loc[p][\"i\"])\n",
    "df[\"#1 ID\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"#2 ID\"] = df[\"#1 ID\"].max() + df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'paragraph', 'answer', 'positive', '#1 ID', '#2 ID'], dtype='object')"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quality</th>\n",
       "      <th>#1 ID</th>\n",
       "      <th>#2 ID</th>\n",
       "      <th>#1 String</th>\n",
       "      <th>#2 String</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1683</td>\n",
       "      <td>18891</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1683</td>\n",
       "      <td>18892</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>statue of Mary.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1683</td>\n",
       "      <td>18893</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1683</td>\n",
       "      <td>18894</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>Mary reputedly appeared to Saint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1683</td>\n",
       "      <td>18895</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Quality  #1 ID  #2 ID                                          #1 String  \\\n",
       "0        1   1683  18891  Architecturally, the school has a Catholic cha...   \n",
       "1        0   1683  18892  Architecturally, the school has a Catholic cha...   \n",
       "2        1   1683  18893  Architecturally, the school has a Catholic cha...   \n",
       "3        0   1683  18894  Architecturally, the school has a Catholic cha...   \n",
       "4        1   1683  18895  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                          #2 String  \n",
       "0        Saint Bernadette Soubirous  \n",
       "1                   statue of Mary.  \n",
       "2         a copper statue of Christ  \n",
       "3  Mary reputedly appeared to Saint  \n",
       "4                 the Main Building  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mrpc = df[['positive', '#1 ID', '#2 ID', 'paragraph', 'answer']]\n",
    "df_mrpc.columns = ['Quality', '#1 ID', '#2 ID', '#1 String', '#2 String']\n",
    "df_mrpc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "df_mrpc.to_csv(\"/home/xstefan3/bert_experiments/pytorch/cc/1_extraction/qp_pairs_data/MRPC_format/train.tsv\", \n",
    "               sep=\"\\t\", index=False, quotechar=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write_tsv(input_file, quotechar=None):\n",
    "    \"\"\"Reads a tab separated value file.\"\"\"\n",
    "    with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        writer = csv.writer(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "        lines = []\n",
    "        for line in df_mrpc.iterrows():\n",
    "            writer.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1130,\n",
       " 1351,\n",
       " 1617,\n",
       " 117,\n",
       " 24041,\n",
       " 1598,\n",
       " 1123,\n",
       " 3176,\n",
       " 1578,\n",
       " 1773,\n",
       " 3977,\n",
       " 16844,\n",
       " 24432,\n",
       " 3338,\n",
       " 2639,\n",
       " 14311,\n",
       " 1107,\n",
       " 1103,\n",
       " 3789,\n",
       " 1273,\n",
       " 117,\n",
       " 5202,\n",
       " 12305,\n",
       " 1107,\n",
       " 3487,\n",
       " 3263,\n",
       " 10615,\n",
       " 117,\n",
       " 1134,\n",
       " 2097,\n",
       " 1157,\n",
       " 1148,\n",
       " 5138,\n",
       " 10343,\n",
       " 1103,\n",
       " 1646,\n",
       " 2884,\n",
       " 1701,\n",
       " 1105,\n",
       " 18758,\n",
       " 109,\n",
       " 5766,\n",
       " 1550,\n",
       " 119,\n",
       " 24041,\n",
       " 1308,\n",
       " 107,\n",
       " 6955,\n",
       " 1135,\n",
       " 3929,\n",
       " 107,\n",
       " 1112,\n",
       " 1103,\n",
       " 1730,\n",
       " 1423,\n",
       " 1121,\n",
       " 1157,\n",
       " 5945,\n",
       " 1312,\n",
       " 1134,\n",
       " 2242,\n",
       " 1103,\n",
       " 1499,\n",
       " 1995,\n",
       " 1107,\n",
       " 1103,\n",
       " 1993,\n",
       " 117,\n",
       " 4323,\n",
       " 117,\n",
       " 1105,\n",
       " 4990,\n",
       " 119,\n",
       " 1130,\n",
       " 1581,\n",
       " 117,\n",
       " 24041,\n",
       " 4950,\n",
       " 3714,\n",
       " 6881,\n",
       " 2750,\n",
       " 1158,\n",
       " 117,\n",
       " 3108,\n",
       " 119,\n",
       " 117,\n",
       " 1107,\n",
       " 1103,\n",
       " 2696,\n",
       " 3789,\n",
       " 1109,\n",
       " 10864,\n",
       " 12008,\n",
       " 18378,\n",
       " 6006,\n",
       " 1112,\n",
       " 15482,\n",
       " 117,\n",
       " 170,\n",
       " 1423,\n",
       " 1534,\n",
       " 2292,\n",
       " 2750,\n",
       " 1158,\n",
       " 112,\n",
       " 188,\n",
       " 1959,\n",
       " 4887,\n",
       " 1107,\n",
       " 1567,\n",
       " 1114,\n",
       " 119,\n",
       " 1109,\n",
       " 1273,\n",
       " 1460,\n",
       " 3216,\n",
       " 3761,\n",
       " 1121,\n",
       " 4217,\n",
       " 1133,\n",
       " 18758,\n",
       " 109,\n",
       " 1476,\n",
       " 1550,\n",
       " 1107,\n",
       " 1103,\n",
       " 158,\n",
       " 119,\n",
       " 156,\n",
       " 119,\n",
       " 24041,\n",
       " 1308,\n",
       " 107,\n",
       " 10864,\n",
       " 12008,\n",
       " 18378,\n",
       " 1891,\n",
       " 107,\n",
       " 1112,\n",
       " 1103,\n",
       " 1730,\n",
       " 1423,\n",
       " 1121,\n",
       " 1103,\n",
       " 1273,\n",
       " 112,\n",
       " 188,\n",
       " 5945,\n",
       " 1312,\n",
       " 117,\n",
       " 1114,\n",
       " 3056,\n",
       " 1183,\n",
       " 9657,\n",
       " 117,\n",
       " 12029,\n",
       " 149,\n",
       " 14300,\n",
       " 117,\n",
       " 1105,\n",
       " 4299,\n",
       " 1134,\n",
       " 1108,\n",
       " 1145,\n",
       " 1215,\n",
       " 1106,\n",
       " 4609,\n",
       " 1103,\n",
       " 1273,\n",
       " 119,\n",
       " 2543,\n",
       " 1104,\n",
       " 24041,\n",
       " 112,\n",
       " 188,\n",
       " 5353,\n",
       " 1106,\n",
       " 1103,\n",
       " 5945,\n",
       " 117,\n",
       " 107,\n",
       " 2659,\n",
       " 4974,\n",
       " 107,\n",
       " 117,\n",
       " 14550,\n",
       " 1181,\n",
       " 1618,\n",
       " 1113,\n",
       " 1103,\n",
       " 1646,\n",
       " 5896,\n",
       " 119]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(df.iloc[789].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(text1, text2, to_len=512):\n",
    "    l = len(tokenizer.encode(text1)) + len(tokenizer.encode(text2))\n",
    "    if l > to_len:\n",
    "        diff = l - to_len\n",
    "        print(\"text1, text2 len: %s, %s\" % (len(text1[:-diff]), len(text2)))\n",
    "        return text1[:-diff], text2\n",
    "    else:\n",
    "        return text1, text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_and_tokenize(text1, text2, add_special_tokens, to_len=512):\n",
    "    text1_tok, text2_tok = tokenizer.encode(text1), tokenizer.encode(text2)\n",
    "    output_tokenized = tokenizer.encode(text1, text2, add_special_tokens=True)\n",
    "    if len(output_tokenized) > to_len:\n",
    "        return None, None\n",
    "        diff = len(output_tokenized) - to_len\n",
    "        # remove prolonged part from the first tokenized text, keeping the special separate tokens\n",
    "        orig_text1_len = len(text2_tok)\n",
    "        output_tokenized = output_tokenized[:orig_text1_len-diff] + output_tokenized[orig_text1_len:]\n",
    "        print(\"Customized len: %s\" % len(output_tokenized))\n",
    "        segment_ids = [0]*(orig_text1_len-diff) + [1]*(len(output_tokenized)-orig_text1_len)\n",
    "        return output_tokenized, segment_ids\n",
    "    else:\n",
    "        segment_ids = [0]*len(text1_tok) + [1]*len(text2_tok)\n",
    "        return output_tokenized, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fucking alignment\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "input_ids = pad_sequences([tokenizer.encode(txt) for txt in indexed_tokens],\n",
    "                          maxlen=512, dtype=\"long\", truncating=\"post\", padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5575, 512)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-bee4aa261efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m for new_tokens, new_segments, l in (strip_and_tokenize(row[1], row[2], add_special_tokens=True) + (row[-1],) \n\u001b[0;32m----> 5\u001b[0;31m                                  for _, row in df.iterrows()):\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mindexed_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-bee4aa261efd>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msegments_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m for new_tokens, new_segments, l in (strip_and_tokenize(row[1], row[2], add_special_tokens=True) + (row[-1],) \n\u001b[0m\u001b[1;32m      5\u001b[0m                                  for _, row in df.iterrows()):\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 data = sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 262\u001b[0;31m                                       raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# we will try to copy be-definition here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_try_cast\u001b[0;34m(arr, take_fast_path, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_cast_to_integer_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m         \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_cast_to_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         \u001b[0;31m# Take care in creating object arrays (but iterators are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;31m# supported):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_cast_to_datetime\u001b[0;34m(value, dtype, errors)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \"\"\"\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedeltas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_timedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetimes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "indexed_tokens = []\n",
    "segments_ids = []\n",
    "labels = []\n",
    "for new_tokens, new_segments, l in (strip_and_tokenize(row[1], row[2], add_special_tokens=True) + (row[-1],) \n",
    "                                 for _, row in df.iterrows()):\n",
    "    if new_tokens is not None:\n",
    "        indexed_tokens.append(new_tokens)\n",
    "        segments_ids.append(new_segments)\n",
    "        labels.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 173 at dim 2 (got 170)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-81e193b6dab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexed_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msegments_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msegments_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 173 at dim 2 (got 170)"
     ]
    }
   ],
   "source": [
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  3104,  1124, 15703,  1108,   170, 16797,  8284,   102,  2627,\n",
       "          1108,  3104,  1124, 15703,   136,   102],\n",
       "        [  101,  3104,  1124, 15703,  1108,   170, 16797,  8284,   102,  2627,\n",
       "          1108,  3104,  1124, 15703,   136,   102],\n",
       "        [  101,  3104,  1124, 15703,  1108,   170, 16797,  8284,   102,  2627,\n",
       "          1108,  3104,  1124, 15703,   136,   102]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[170,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 166,\n",
       " 170,\n",
       " 170,\n",
       " 173,\n",
       " 170,\n",
       " 175,\n",
       " 265,\n",
       " 265,\n",
       " 264,\n",
       " 264,\n",
       " 265,\n",
       " 265,\n",
       " 264,\n",
       " 264,\n",
       " 264,\n",
       " 264,\n",
       " 152,\n",
       " 152,\n",
       " 154,\n",
       " 153,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 161,\n",
       " 160,\n",
       " 160,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 143,\n",
       " 147,\n",
       " 140,\n",
       " 140,\n",
       " 141,\n",
       " 141,\n",
       " 133,\n",
       " 133,\n",
       " 131,\n",
       " 134,\n",
       " 136,\n",
       " 138,\n",
       " 138,\n",
       " 135,\n",
       " 262,\n",
       " 262,\n",
       " 264,\n",
       " 264,\n",
       " 265,\n",
       " 265,\n",
       " 262,\n",
       " 262,\n",
       " 267,\n",
       " 265,\n",
       " 135,\n",
       " 132,\n",
       " 132,\n",
       " 133,\n",
       " 125,\n",
       " 125,\n",
       " 128,\n",
       " 126,\n",
       " 127,\n",
       " 125,\n",
       " 107,\n",
       " 107,\n",
       " 113,\n",
       " 110,\n",
       " 107,\n",
       " 107,\n",
       " 110,\n",
       " 108,\n",
       " 109,\n",
       " 111,\n",
       " 210,\n",
       " 207,\n",
       " 210,\n",
       " 207,\n",
       " 214,\n",
       " 219,\n",
       " 210,\n",
       " 207,\n",
       " 210,\n",
       " 210,\n",
       " 256,\n",
       " 256,\n",
       " 255,\n",
       " 255,\n",
       " 256,\n",
       " 256,\n",
       " 256,\n",
       " 260,\n",
       " 258,\n",
       " 255,\n",
       " 213,\n",
       " 213,\n",
       " 206,\n",
       " 204,\n",
       " 206,\n",
       " 207,\n",
       " 207,\n",
       " 206,\n",
       " 208,\n",
       " 208,\n",
       " 111,\n",
       " 111,\n",
       " 113,\n",
       " 113,\n",
       " 112,\n",
       " 112,\n",
       " 116,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 259,\n",
       " 257,\n",
       " 256,\n",
       " 256,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 255,\n",
       " 136,\n",
       " 139,\n",
       " 135,\n",
       " 137,\n",
       " 133,\n",
       " 133,\n",
       " 135,\n",
       " 137,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 174,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 339,\n",
       " 336,\n",
       " 339,\n",
       " 336,\n",
       " 337,\n",
       " 337,\n",
       " 342,\n",
       " 341,\n",
       " 338,\n",
       " 338,\n",
       " 250,\n",
       " 249,\n",
       " 249,\n",
       " 249,\n",
       " 249,\n",
       " 249,\n",
       " 250,\n",
       " 250,\n",
       " 249,\n",
       " 249,\n",
       " 288,\n",
       " 280,\n",
       " 280,\n",
       " 283,\n",
       " 279,\n",
       " 279,\n",
       " 279,\n",
       " 277,\n",
       " 279,\n",
       " 278,\n",
       " 195,\n",
       " 195,\n",
       " 194,\n",
       " 194,\n",
       " 198,\n",
       " 197,\n",
       " 196,\n",
       " 196,\n",
       " 199,\n",
       " 196,\n",
       " 164,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 164,\n",
       " 161,\n",
       " 161,\n",
       " 162,\n",
       " 166,\n",
       " 181,\n",
       " 178,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 177,\n",
       " 328,\n",
       " 328,\n",
       " 331,\n",
       " 333,\n",
       " 329,\n",
       " 330,\n",
       " 332,\n",
       " 328,\n",
       " 327,\n",
       " 328,\n",
       " 174,\n",
       " 172,\n",
       " 174,\n",
       " 174,\n",
       " 172,\n",
       " 170,\n",
       " 169,\n",
       " 169,\n",
       " 169,\n",
       " 169,\n",
       " 230,\n",
       " 230,\n",
       " 234,\n",
       " 236,\n",
       " 231,\n",
       " 231,\n",
       " 242,\n",
       " 236,\n",
       " 229,\n",
       " 229,\n",
       " 138,\n",
       " 136,\n",
       " 136,\n",
       " 136,\n",
       " 138,\n",
       " 137,\n",
       " 138,\n",
       " 137,\n",
       " 136,\n",
       " 140,\n",
       " 306,\n",
       " 304,\n",
       " 307,\n",
       " 308,\n",
       " 307,\n",
       " 308,\n",
       " 303,\n",
       " 303,\n",
       " 307,\n",
       " 306,\n",
       " 175,\n",
       " 173,\n",
       " 174,\n",
       " 173,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 176,\n",
       " 173,\n",
       " 175,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 130,\n",
       " 131,\n",
       " 130,\n",
       " 133,\n",
       " 134,\n",
       " 132,\n",
       " 130,\n",
       " 199,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 197,\n",
       " 199,\n",
       " 198,\n",
       " 197,\n",
       " 196,\n",
       " 196,\n",
       " 166,\n",
       " 165,\n",
       " 162,\n",
       " 162,\n",
       " 166,\n",
       " 164,\n",
       " 166,\n",
       " 165,\n",
       " 166,\n",
       " 166,\n",
       " 349,\n",
       " 349,\n",
       " 353,\n",
       " 351,\n",
       " 351,\n",
       " 351,\n",
       " 350,\n",
       " 350,\n",
       " 352,\n",
       " 351,\n",
       " 182,\n",
       " 182,\n",
       " 183,\n",
       " 182,\n",
       " 185,\n",
       " 187,\n",
       " 189,\n",
       " 188,\n",
       " 120,\n",
       " 120,\n",
       " 129,\n",
       " 129,\n",
       " 122,\n",
       " 122,\n",
       " 120,\n",
       " 120,\n",
       " 126,\n",
       " 128,\n",
       " 121,\n",
       " 122,\n",
       " 121,\n",
       " 123,\n",
       " 123,\n",
       " 124,\n",
       " 121,\n",
       " 121,\n",
       " 132,\n",
       " 132,\n",
       " 130,\n",
       " 131,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 135,\n",
       " 133,\n",
       " 132,\n",
       " 121,\n",
       " 121,\n",
       " 120,\n",
       " 124,\n",
       " 121,\n",
       " 119,\n",
       " 119,\n",
       " 119,\n",
       " 123,\n",
       " 123,\n",
       " 140,\n",
       " 140,\n",
       " 145,\n",
       " 145,\n",
       " 143,\n",
       " 143,\n",
       " 144,\n",
       " 144,\n",
       " 186,\n",
       " 194,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 189,\n",
       " 182,\n",
       " 180,\n",
       " 181,\n",
       " 181,\n",
       " 126,\n",
       " 126,\n",
       " 124,\n",
       " 126,\n",
       " 126,\n",
       " 127,\n",
       " 124,\n",
       " 124,\n",
       " 124,\n",
       " 122,\n",
       " 150,\n",
       " 150,\n",
       " 150,\n",
       " 147,\n",
       " 148,\n",
       " 148,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 146,\n",
       " 240,\n",
       " 242,\n",
       " 237,\n",
       " 237,\n",
       " 241,\n",
       " 241,\n",
       " 239,\n",
       " 239,\n",
       " 242,\n",
       " 240,\n",
       " 122,\n",
       " 122,\n",
       " 133,\n",
       " 127,\n",
       " 127,\n",
       " 131,\n",
       " 125,\n",
       " 124,\n",
       " 130,\n",
       " 127,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 116,\n",
       " 115,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 114,\n",
       " 118,\n",
       " 117,\n",
       " 116,\n",
       " 117,\n",
       " 119,\n",
       " 115,\n",
       " 117,\n",
       " 115,\n",
       " 111,\n",
       " 110,\n",
       " 112,\n",
       " 111,\n",
       " 112,\n",
       " 111,\n",
       " 113,\n",
       " 113,\n",
       " 112,\n",
       " 110,\n",
       " 211,\n",
       " 214,\n",
       " 209,\n",
       " 209,\n",
       " 211,\n",
       " 211,\n",
       " 209,\n",
       " 212,\n",
       " 210,\n",
       " 213,\n",
       " 337,\n",
       " 337,\n",
       " 336,\n",
       " 336,\n",
       " 339,\n",
       " 340,\n",
       " 340,\n",
       " 340,\n",
       " 340,\n",
       " 336,\n",
       " 197,\n",
       " 196,\n",
       " 198,\n",
       " 197,\n",
       " 195,\n",
       " 195,\n",
       " 204,\n",
       " 209,\n",
       " 198,\n",
       " 199,\n",
       " 179,\n",
       " 178,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 179,\n",
       " 175,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 322,\n",
       " 320,\n",
       " 321,\n",
       " 325,\n",
       " 322,\n",
       " 321,\n",
       " 321,\n",
       " 319,\n",
       " 322,\n",
       " 319,\n",
       " 155,\n",
       " 154,\n",
       " 156,\n",
       " 154,\n",
       " 156,\n",
       " 156,\n",
       " 163,\n",
       " 164,\n",
       " 152,\n",
       " 152,\n",
       " 387,\n",
       " 385,\n",
       " 384,\n",
       " 384,\n",
       " 384,\n",
       " 384,\n",
       " 385,\n",
       " 385,\n",
       " 386,\n",
       " 385,\n",
       " 163,\n",
       " 162,\n",
       " 160,\n",
       " 160,\n",
       " 165,\n",
       " 165,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 162,\n",
       " 161,\n",
       " 159,\n",
       " 160,\n",
       " 158,\n",
       " 160,\n",
       " 159,\n",
       " 161,\n",
       " 161,\n",
       " 162,\n",
       " 160,\n",
       " 266,\n",
       " 263,\n",
       " 268,\n",
       " 268,\n",
       " 265,\n",
       " 264,\n",
       " 263,\n",
       " 263,\n",
       " 265,\n",
       " 265,\n",
       " 165,\n",
       " 167,\n",
       " 164,\n",
       " 168,\n",
       " 162,\n",
       " 162,\n",
       " 164,\n",
       " 164,\n",
       " 163,\n",
       " 163,\n",
       " 165,\n",
       " 163,\n",
       " 165,\n",
       " 167,\n",
       " 164,\n",
       " 163,\n",
       " 162,\n",
       " 164,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 164,\n",
       " 167,\n",
       " 167,\n",
       " 163,\n",
       " 165,\n",
       " 163,\n",
       " 163,\n",
       " 165,\n",
       " 166,\n",
       " 162,\n",
       " 163,\n",
       " 162,\n",
       " 162,\n",
       " 163,\n",
       " 163,\n",
       " 165,\n",
       " 164,\n",
       " 162,\n",
       " 162,\n",
       " 242,\n",
       " 242,\n",
       " 243,\n",
       " 243,\n",
       " 242,\n",
       " 242,\n",
       " 244,\n",
       " 242,\n",
       " 242,\n",
       " 242,\n",
       " 242,\n",
       " 242,\n",
       " 243,\n",
       " 243,\n",
       " 243,\n",
       " 243,\n",
       " 244,\n",
       " 242,\n",
       " 244,\n",
       " 242,\n",
       " 243,\n",
       " 243,\n",
       " 245,\n",
       " 243,\n",
       " 221,\n",
       " 217,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 217,\n",
       " 216,\n",
       " 216,\n",
       " 217,\n",
       " 215,\n",
       " 215,\n",
       " 215,\n",
       " 216,\n",
       " 215,\n",
       " 215,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 214,\n",
       " 151,\n",
       " 149,\n",
       " 153,\n",
       " 155,\n",
       " 150,\n",
       " 150,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 148,\n",
       " 153,\n",
       " 151,\n",
       " 150,\n",
       " 148,\n",
       " 152,\n",
       " 153,\n",
       " 150,\n",
       " 149,\n",
       " 148,\n",
       " 150,\n",
       " 150,\n",
       " 148,\n",
       " 153,\n",
       " 151,\n",
       " 148,\n",
       " 148,\n",
       " 165,\n",
       " 164,\n",
       " 166,\n",
       " 165,\n",
       " 164,\n",
       " 166,\n",
       " 168,\n",
       " 168,\n",
       " 171,\n",
       " 168,\n",
       " 166,\n",
       " 166,\n",
       " 164,\n",
       " 166,\n",
       " 165,\n",
       " 164,\n",
       " 166,\n",
       " 168,\n",
       " 164,\n",
       " 164,\n",
       " 171,\n",
       " 171,\n",
       " 294,\n",
       " 292,\n",
       " 294,\n",
       " 292,\n",
       " 294,\n",
       " 292,\n",
       " 294,\n",
       " 292,\n",
       " 291,\n",
       " 291,\n",
       " 292,\n",
       " 295,\n",
       " 294,\n",
       " 292,\n",
       " 292,\n",
       " 293,\n",
       " 291,\n",
       " 291,\n",
       " 295,\n",
       " 292,\n",
       " 294,\n",
       " 292,\n",
       " 291,\n",
       " 291,\n",
       " 300,\n",
       " 295,\n",
       " 329,\n",
       " 329,\n",
       " 331,\n",
       " 329,\n",
       " 328,\n",
       " 328,\n",
       " 327,\n",
       " 327,\n",
       " 329,\n",
       " 329,\n",
       " 329,\n",
       " 329,\n",
       " 329,\n",
       " 331,\n",
       " 328,\n",
       " 328,\n",
       " 330,\n",
       " 330,\n",
       " 330,\n",
       " 330,\n",
       " 331,\n",
       " 331,\n",
       " 327,\n",
       " 329,\n",
       " 328,\n",
       " 328,\n",
       " 162,\n",
       " 162,\n",
       " 164,\n",
       " 165,\n",
       " 163,\n",
       " 163,\n",
       " 168,\n",
       " 166,\n",
       " 165,\n",
       " 166,\n",
       " 163,\n",
       " 163,\n",
       " 168,\n",
       " 166,\n",
       " 162,\n",
       " 162,\n",
       " 163,\n",
       " 163,\n",
       " 164,\n",
       " 163,\n",
       " 253,\n",
       " 253,\n",
       " 250,\n",
       " 250,\n",
       " 250,\n",
       " 250,\n",
       " 254,\n",
       " 252,\n",
       " 253,\n",
       " 251,\n",
       " 250,\n",
       " 252,\n",
       " 254,\n",
       " 251,\n",
       " 255,\n",
       " 257,\n",
       " 250,\n",
       " 250,\n",
       " 254,\n",
       " 252,\n",
       " 251,\n",
       " 251,\n",
       " 195,\n",
       " 195,\n",
       " 199,\n",
       " 201,\n",
       " 198,\n",
       " 196,\n",
       " 196,\n",
       " 195,\n",
       " 195,\n",
       " 194,\n",
       " 199,\n",
       " 197,\n",
       " 195,\n",
       " 195,\n",
       " 195,\n",
       " 195,\n",
       " 197,\n",
       " 197,\n",
       " 195,\n",
       " 196,\n",
       " 199,\n",
       " 198,\n",
       " 196,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 197,\n",
       " 196,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 263,\n",
       " 264,\n",
       " 264,\n",
       " 266,\n",
       " 262,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 264,\n",
       " 263,\n",
       " 263,\n",
       " 265,\n",
       " 266,\n",
       " 263,\n",
       " 263,\n",
       " 265,\n",
       " 267,\n",
       " 264,\n",
       " 264,\n",
       " 266,\n",
       " 263,\n",
       " 263,\n",
       " 264,\n",
       " 278,\n",
       " 275,\n",
       " 274,\n",
       " 274,\n",
       " 275,\n",
       " 275,\n",
       " 278,\n",
       " 275,\n",
       " 274,\n",
       " 274,\n",
       " 275,\n",
       " 275,\n",
       " 278,\n",
       " 277,\n",
       " 279,\n",
       " 279,\n",
       " 280,\n",
       " 276,\n",
       " 279,\n",
       " 276,\n",
       " 168,\n",
       " 165,\n",
       " 170,\n",
       " 167,\n",
       " 165,\n",
       " 167,\n",
       " 165,\n",
       " 166,\n",
       " 168,\n",
       " 166,\n",
       " 166,\n",
       " 169,\n",
       " 166,\n",
       " 168,\n",
       " 167,\n",
       " 165,\n",
       " 168,\n",
       " 165,\n",
       " 166,\n",
       " 168,\n",
       " 166,\n",
       " 167,\n",
       " 202,\n",
       " 204,\n",
       " 202,\n",
       " 203,\n",
       " 202,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 201,\n",
       " 202,\n",
       " 202,\n",
       " 202,\n",
       " 200,\n",
       " 202,\n",
       " 203,\n",
       " 202,\n",
       " 201,\n",
       " 200,\n",
       " 200,\n",
       " 202,\n",
       " 202,\n",
       " 202,\n",
       " 200,\n",
       " 450,\n",
       " 450,\n",
       " 452,\n",
       " 451,\n",
       " 449,\n",
       " 449,\n",
       " 450,\n",
       " 450,\n",
       " 452,\n",
       " 451,\n",
       " 453,\n",
       " 453,\n",
       " 450,\n",
       " 451,\n",
       " 450,\n",
       " 453,\n",
       " 452,\n",
       " 450,\n",
       " 452,\n",
       " 453,\n",
       " 451,\n",
       " 450,\n",
       " 452,\n",
       " 451,\n",
       " 450,\n",
       " 450,\n",
       " 452,\n",
       " 450,\n",
       " 233,\n",
       " 232,\n",
       " 232,\n",
       " 232,\n",
       " 232,\n",
       " 234,\n",
       " 231,\n",
       " 231,\n",
       " 236,\n",
       " 239,\n",
       " 232,\n",
       " 235,\n",
       " 233,\n",
       " 231,\n",
       " 232,\n",
       " 232,\n",
       " 232,\n",
       " 232,\n",
       " 233,\n",
       " 232,\n",
       " 232,\n",
       " 232,\n",
       " 241,\n",
       " 236,\n",
       " 234,\n",
       " 231,\n",
       " 161,\n",
       " 161,\n",
       " 163,\n",
       " 163,\n",
       " 162,\n",
       " 162,\n",
       " 161,\n",
       " 161,\n",
       " 163,\n",
       " 163,\n",
       " 162,\n",
       " 162,\n",
       " 161,\n",
       " 161,\n",
       " 162,\n",
       " 162,\n",
       " 163,\n",
       " 165,\n",
       " 163,\n",
       " 162,\n",
       " 161,\n",
       " 161,\n",
       " 163,\n",
       " 162,\n",
       " 162,\n",
       " 162,\n",
       " 163,\n",
       " 162,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 86,\n",
       " 90,\n",
       " 90,\n",
       " 86,\n",
       " 86,\n",
       " 87,\n",
       " 87,\n",
       " 88,\n",
       " 88,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 88,\n",
       " 87,\n",
       " 89,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 87,\n",
       " 123,\n",
       " 123,\n",
       " ...]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len, segments_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}]\n",
    "\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [00:00<00:06,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.5352987051010132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 2/10 [00:01<00:05,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.4916340112686157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 3/10 [00:02<00:05,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.5953325033187866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 4/10 [00:03<00:04,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.3386849164962769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 5/10 [00:03<00:03,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.4419488906860352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 6/10 [00:04<00:02,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.646873116493225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 7/10 [00:05<00:02,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.5695792436599731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 8/10 [00:05<00:01,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.4612122774124146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 9/10 [00:06<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.4240833520889282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [00:07<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 1.3668159246444702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, trange\n",
    "epochs = 10\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate([tokens_tensor]):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "#         segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "#         segments_tensors = torch.tensor([segments_ids]*3)\n",
    "#         tokens_tensor = torch.tensor([indexed_tokens]*3)\n",
    "#         labels = torch.tensor([1]*3)\n",
    "        # forward pass\n",
    "        loss, _ = model(tokens_tensor, token_type_ids=segments_tensors, labels=labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "#         nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_examples += 1\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"\\nTrain loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "#     # VALIDATION on validation set\n",
    "#     model.eval()\n",
    "#     eval_loss, eval_accuracy = 0, 0\n",
    "#     nb_eval_steps, nb_eval_examples = 0, 0\n",
    "#     predictions , true_labels = [], []\n",
    "#     for batch in [tokens_tensor]:\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "# #         segments_ids, segments_tensors, tokens_tensor = batch\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             tmp_eval_loss, logits = model(tokens_tensor, token_type_ids=segments_tensors, labels=labels)\n",
    "# #             logits = model(b_input_ids, token_type_ids=None,\n",
    "# #                            attention_mask=b_input_mask)\n",
    "#         logits = logits.detach().cpu().numpy()\n",
    "#         label_ids = b_labels.to('cpu').numpy()\n",
    "#         predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "#         true_labels.append(label_ids)\n",
    "        \n",
    "#         tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "#         eval_loss += tmp_eval_loss.mean().item()\n",
    "#         eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "#         nb_eval_examples += b_input_ids.size(0)\n",
    "#         nb_eval_steps += 1\n",
    "#     eval_loss = eval_loss/nb_eval_steps\n",
    "#     print(\"Validation loss: {}\".format(eval_loss))\n",
    "#     print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "#     pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "#     valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "#     print(\"F1-Score: {}\".format(f1_score(valid_tags, pred_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fc99547240e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number"
     ]
    }
   ],
   "source": [
    "loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
